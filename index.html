<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 
<link href="https://fonts.googleapis.com/css2?family=Lato&display=swap"
      rel="stylesheet">
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
	<title>Learning Shared Safety Constraints from Multi-task Demonstrations</title>
    <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
        if you update and want to force Facebook to re-scrape. -->
	<meta property="og:title" content="Learning Shared Safety Constraints from Multi-task Demonstrations" />
	<meta property="og:description" content="We formalize the problem of learning constraints from expert demonstrations by extending inverse reinforcement learning, and we develop a multi-task version of inverse constraint learning to avoid selecting degenerate constraints. We validate our methods on high-dimensional continuous control tasks and show that we can match expert performance and recover ground-truth constraints. " />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="We formalize the problem of learning constraints from expert demonstrations by extending inverse reinforcement learning, and we develop a multi-task version of inverse constraint learning to avoid selecting degenerate constraints. We validate our methods on high-dimensional continuous control tasks and show that we can match expert performance and recover ground-truth constraints. " />
    <meta property="twitter:title"         content="Learning Shared Safety Constraints from Multi-task Demonstrations" />
    <meta property="twitter:description"   content="We formalize the problem of learning constraints from expert demonstrations by extending inverse reinforcement learning, and we develop a multi-task version of inverse constraint learning to avoid selecting degenerate constraints. We validate our methods on high-dimensional continuous control tasks and show that we can match expert performance and recover ground-truth constraints. " />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

</head>

<body>
<div class="container">
    <div class="title">
        Learning Shared Safety Constraints <br> from Multi-task Demonstrations
    </div>

    <div class="venue">
        NeurIPS '23
    </div>

    <br><br>
    <div class="author">
        <a href="https://konwook.github.io/">Konwoo Kim</a><sup>*, 1</sup>
    </div>
    <div class="author">
        <a href="https://gokul.dev/">Gokul Swamy</a><sup>*, 1</sup>
    </div>
    <div class="author">
        <a href="https://zuxin.me/">Zuxin Liu</a><sup>1</sup>
    </div>
    <br>
    <div class="author">
        <a href="https://www.meche.engineering.cmu.edu/directory/bios/zhao-ding.html">Ding Zhao</a><sup>1</sup>
    </div>
    <div class="author">
        <a href="http://www.sanjibanchoudhury.com/">Sanjiban Choudhury</a><sup>2</sup>
    </div>
    <div class="author">
        <a href="https://zstevenwu.com/">Steven Wu</a><sup>1</sup>
    </div>

    <br><br>

    <div class="affiliation"><sup>1&nbsp;</sup>CMU</div>
    <div class="affiliation"><sup>2&nbsp;</sup>Cornell University</div>
    

    <br><br>

    <div class="links"><a href="https://arxiv.org/pdf/2309.00711.pdf"><i class="fa fa-file-text", style="font-size: 50px; padding-bottom: 10px"></i><br>[Paper]</a></div>
    <div class="links"><a href="https://www.youtube.com/watch?v=2jFMBBbxcuY"><i class="fa fa-play-circle" style="font-size: 50px; padding-bottom: 10px"></i><br>[Video]</a></div>
    <div class="links"><a href="https://github.com/konwook/mticl"><i class="fa fa-github" style="font-size: 50px; padding-bottom: 10px"></i><br>[Code]</a></div>

    <br><br>

    <img style="width: 60%;" src="./resources/icl_ffig.svg" alt="Teaser figure."/>
    <br>
    <p style="width: 80%;">
      <br> We formalize the problem of learning constraints from expert demonstrations by extending inverse reinforcement learning, and we develop a multi-task version of inverse constraint learning to avoid selecting degenerate constraints. We validate our methods on high-dimensional continuous control tasks and show that we can match expert performance and recover ground-truth constraints. 
    </p>
    <hr>

    <h1>Abstract</h1>
    <p style="width: 80%;">
	    Regardless of the particular task we want them to perform in an environment, there are often shared <i>safety constraints</i> we want our agents to respect. For example, regardless of whether it is making a sandwich or clearing the table, a kitchen robot should not break a plate. Manually specifying such a constraint can be both time-consuming and error-prone. We show how to learn constraints from expert demonstrations of safe task completion by extending inverse reinforcement learning (IRL) techniques to the space of constraints. Intuitively, we learn constraints that forbid highly rewarding behavior that the expert could have taken but chose not to. Unfortunately, the constraint learning problem is rather ill-posed and typically leads to overly conservative constraints that forbid all behavior that the expert did not take. We counter this by leveraging diverse demonstrations that naturally occur in multi-task settings to learn a tighter set of constraints. We validate our method with simulation experiments on high-dimensional continuous control tasks.
    </p>
    <br>
    <hr>

    <h1>Video</h1>
    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/2jFMBBbxcuY" frameBorder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div>
    <br>

    <hr>

    <h1>Key Insights</h1>

    <h2>1. Formalizing Inverse Constraint Learning </h2>
    <p style="width: 80%;">
        We consider a setting where we have access to expert demonstrations of a task, along with the task's reward. 
        This allows us to compare the behavior of the expert and reward-optimal policy for a task. Our first insight is that actions taken by the reward-optimal but not the expert policy are likely to be forbidden. 
        We can extract a constraint in this way by formulating <code>ICL</code> (inverse constraint learning) as a two-player zero sum game between a constraint and policy player:
        $$\color[HTML]{F96565}{\max_{c \in \mathcal{F}_c} \max_{\lambda > 0}}\color[HTML]{008000}{\min_{\pi \in \Pi}} J(\pi_E, r - \color[HTML]{F96565}{\lambda c}) - J(\color[HTML]{008000}{\pi}, r - \color[HTML]{F96565}{\lambda c})$$
    </p>

    <p style="width: 80%;">
        For a fixed constraint, the policy player aims to maximize their reward while satisfying the constraint by solving constrained RL: 
        $$ \color[HTML]{F96565}{\max_{\lambda > 0}}\color[HTML]{008000}{\min_{\pi \in \Pi}} -J(\color{green}{\pi}, r - \color[HTML]{F96565}{\lambda c})$$ 
    </p>
    <p style="width: 80%;">
        For a fixed policy, the constraint player uses classification to pick the constraint that maximally penalizes the learner relative to the expert: 
        $$\color[HTML]{F96565}{\max_{c \in \mathcal{F}_c}} J(\pi_E, -\color[HTML]{F96565}{\lambda c}) - J(\color[HTML]{008000}{\pi}, -\color[HTML]{F96565}{\lambda c})$$
    </p>

    <h2>2. Leveraging Multi-task Data </h2>
    <p style="width: 80%;"> One potential failure mode of <code>ICL</code> is that it can lead to an overly conservative constraint which forbids all non-expert behavior. Such a constraint would fail to generalize to new tasks. To resolve this, we propose a multi-task version of inverse constraint learning, <code>MT-ICL</code>, which provides better coverage of the state space and learns a shared constraint across multiple tasks.
    If we observe $K$ samples of the form $(r_k, \{\xi \sim \pi_E^k \})$, we can formulate the multi-task game as:  </p>
    $$\color[HTML]{F96565}{\max_{c \in \mathcal{F}_c}}\color[HTML]{008000}{\min_{\pi^{1:K} \in \Pi}}\color[HTML]{F96565}{\max_{\lambda^{1:K} > 0}}  \sum_i^K J(\pi_E^i, r^i - \color[HTML]{F96565}{\lambda^i c}) - J(\color{green}{\pi^i}, r^i - \color[HTML]{F96565}{\lambda^i c})$$
    <p style="width: 80%;"> We further provide a statistical condition of generalization of the shared constraint and empirically show that <code>MT-ICL</code> generalizes better to new tasks.</p>


    <h2>3. Practical Procedures for Continuous Control </h2>
    <p style="width: 80%;">We provide implementations of constrained reinforcement learning and inverse constraint learning and benchmark them on tasks from the PyBullet and MuJoCo suite. For a single task with restricted function classes of linear constraints, we show that <code>ICL</code> can exactly recover the ground truth constraint, match expert performance and constraint satisfaction, and is even robust to suboptimal expert demonstrations. </p>
    <img style="width: 20%;" src="./resources/Position_constraint_anneal_aug_slope_0.5.svg" alt="Position constraint figure."/>
    <img style="width: 20%;" src="./resources/Position_reward_anneal_aug_slope_0.5.svg" alt="Position reward figure."/>
    <img style="width: 20%;" src="./resources/Position_violation_anneal_aug_slope_0.5.svg" alt="Position violation figure."/>
    <p style="width: 80%;">
        We consider the task of ant locomotion with a position constraint of staying above the line $y=0.5x$. 
        Over the course of <code>ICL</code> training, the learned position constraint (blue line) converges to the ground truth constraint (red line), and the ant learns to escape the unsafe red region.
    </p>
    <img style="width: 50%; padding-bottom: 20px" src="./resources/evolution_actual_final.svg" alt="Position constraint evolution."/>

    <p style="width: 80%;">
        We evaluate <code>MT-ICL</code> on the more challenging AntMaze setting where navigating to different goals corresponds to different tasks. The constraint in this setting is to not walk through the 
        walls of the maze. Within a single iteration, <code>MT-ICL</code> is able to learn policies that match expert performance and constraint violation, <i>all without ever interacting with the ground-truth maze</i>. 
    </p>
    <img style="width: 20%;" src="./resources/Maze_constraint.svg" alt="Maze constraint figure."/>
    <img style="width: 20%;" src="./resources/Maze_reward.svg" alt="Maze reward figure."/>
    <img style="width: 20%;" src="./resources/Maze_violation.svg" alt="Maze violation figure."/>

    <p style="width: 80%;">
        We visualize our trained policy and the output of our constraint network below.
    </p>
    <img style="width: 40%; padding-bottom: 20px" src="./resources/ant_maze_crop.gif" alt="Ant maze gif."/>

    <p style="width: 80%;"> We release all of our code at the link below. </p>
    <a href="https://github.com/konwook/mticl"><i class="fa fa-github" style="font-size: 50px; padding-bottom: 10px"></i><br>[Code]</a>
    <hr>



    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://arxiv.org/pdf/2309.00711.pdf">
            <img class="layered-paper-big" width="100%" src="./resources/paper.svg" alt="Paper thumbnail"/>
        </a>
    </div>
    <div class="paper-info">
        <h3>Learning Shared Safety Constraints from Multi-task Demonstrations</h3>
        <p> Konwoo Kim<sup>*</sup>, Gokul Swamy<sup>*</sup>, Zuxin Liu, Ding Zhao, Sanjiban Choudhury, Zhiwei Steven Wu</p>
        <pre><code>@inproceedings{kim2023learning,
    title={Learning Shared Safety Constraints from Multi-task Demonstrations},
    author={Konwoo Kim and Gokul Swamy and Zuxin Liu and Ding Zhao and Sanjiban Choudhury and Steven Wu},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=8U31BCquNF}
}</code></pre>
    </div>

    <br>
    <hr>

    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>
        and <a href="http://richzhang.github.io/">Richard Zhang</a> for a
        <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project, and
        adapted to be mobile responsive by <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a>.
        The code we built on can be found <a href="https://github.com/elliottwu/webpage-template">here</a>.
    </p>

    <br>
</div>

</body>

</html>
